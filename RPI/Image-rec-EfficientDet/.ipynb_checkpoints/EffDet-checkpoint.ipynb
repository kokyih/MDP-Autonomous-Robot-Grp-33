{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import argparse\n",
    "import os \n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageDraw\n",
    "from bs4 import BeautifulSoup\n",
    "from imutils import paths\n",
    "\n",
    "import threading\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
    "import torch.optim as optim # All optimization algos, SGD, Adam, etc\n",
    "import torchvision.transforms as transforms # Transformations to perform on data set, in addition to obspy \n",
    "from torch.utils.data import DataLoader # For dataset management and to create mini batches\n",
    "from pprint import pprint\n",
    "\n",
    "import torchvision.datasets as datasets # Standard datasets to test out model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1366\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2732"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_PATH = Path(r\"C:\\Users\\wilso\\Desktop\\MDP-Autonomous-Robot-Grp-33\\RPI\\Image-rec-EfficientDet\\data\\train\")\n",
    "IMAGE_PATHS = list(paths.list_images(TRAIN_PATH))\n",
    "print(len(IMAGE_PATHS))\n",
    "len(list(TRAIN_PATH.glob(\"*\")))\n",
    "# DATA_PATH = r\"C:\\Users\\wilso\\Desktop\\ML-Earthquake-source-physics-research\\src\\data extraction\\wilson\\data_ISC\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1366\n",
      "0 done\n",
      "10 done\n",
      "20 done\n",
      "[!ERROR] C:\\Users\\wilso\\Desktop\\MDP-Autonomous-Robot-Grp-33\\RPI\\Image-rec-EfficientDet\\data\\train\\0.1_img5.jpg\n",
      "30 done\n",
      "[!ERROR] C:\\Users\\wilso\\Desktop\\MDP-Autonomous-Robot-Grp-33\\RPI\\Image-rec-EfficientDet\\data\\train\\0.2.3_img1.jpg\n",
      "[!ERROR] C:\\Users\\wilso\\Desktop\\MDP-Autonomous-Robot-Grp-33\\RPI\\Image-rec-EfficientDet\\data\\train\\0.2.4_img3.jpg\n",
      "40 done\n",
      "[!ERROR] C:\\Users\\wilso\\Desktop\\MDP-Autonomous-Robot-Grp-33\\RPI\\Image-rec-EfficientDet\\data\\train\\20200828_090054.jpg\n",
      "50 done\n",
      "[!ERROR] C:\\Users\\wilso\\Desktop\\MDP-Autonomous-Robot-Grp-33\\RPI\\Image-rec-EfficientDet\\data\\train\\20200828_090348.jpg\n",
      "60 done\n",
      "[!ERROR] C:\\Users\\wilso\\Desktop\\MDP-Autonomous-Robot-Grp-33\\RPI\\Image-rec-EfficientDet\\data\\train\\20200828_090727.jpg\n",
      "[!ERROR] C:\\Users\\wilso\\Desktop\\MDP-Autonomous-Robot-Grp-33\\RPI\\Image-rec-EfficientDet\\data\\train\\20200828_090844.jpg\n",
      "70 done\n",
      "[!ERROR] C:\\Users\\wilso\\Desktop\\MDP-Autonomous-Robot-Grp-33\\RPI\\Image-rec-EfficientDet\\data\\train\\20200828_090846.jpg\n",
      "[!ERROR] C:\\Users\\wilso\\Desktop\\MDP-Autonomous-Robot-Grp-33\\RPI\\Image-rec-EfficientDet\\data\\train\\20200828_090852.jpg\n",
      "[!ERROR] C:\\Users\\wilso\\Desktop\\MDP-Autonomous-Robot-Grp-33\\RPI\\Image-rec-EfficientDet\\data\\train\\20200828_091103.jpg\n",
      "80 done\n",
      "[!ERROR] C:\\Users\\wilso\\Desktop\\MDP-Autonomous-Robot-Grp-33\\RPI\\Image-rec-EfficientDet\\data\\train\\20200828_091114.jpg\n",
      "[!ERROR] C:\\Users\\wilso\\Desktop\\MDP-Autonomous-Robot-Grp-33\\RPI\\Image-rec-EfficientDet\\data\\train\\20200828_091138.jpg\n",
      "[!ERROR] C:\\Users\\wilso\\Desktop\\MDP-Autonomous-Robot-Grp-33\\RPI\\Image-rec-EfficientDet\\data\\train\\20200828_091153.jpg\n",
      "[!ERROR] C:\\Users\\wilso\\Desktop\\MDP-Autonomous-Robot-Grp-33\\RPI\\Image-rec-EfficientDet\\data\\train\\20200828_091357.jpg\n",
      "90 done\n",
      "[!ERROR] C:\\Users\\wilso\\Desktop\\MDP-Autonomous-Robot-Grp-33\\RPI\\Image-rec-EfficientDet\\data\\train\\20200828_091400.jpg\n",
      "[!ERROR] C:\\Users\\wilso\\Desktop\\MDP-Autonomous-Robot-Grp-33\\RPI\\Image-rec-EfficientDet\\data\\train\\20200828_091409.jpg\n",
      "[!ERROR] C:\\Users\\wilso\\Desktop\\MDP-Autonomous-Robot-Grp-33\\RPI\\Image-rec-EfficientDet\\data\\train\\20200828_091420.jpg\n",
      "100 done\n",
      "110 done\n",
      "120 done\n",
      "130 done\n",
      "140 done\n",
      "150 done\n",
      "160 done\n",
      "170 done\n",
      "180 done\n",
      "190 done\n",
      "200 done\n",
      "210 done\n",
      "220 done\n",
      "230 done\n",
      "240 done\n",
      "250 done\n",
      "260 done\n",
      "270 done\n",
      "280 done\n",
      "290 done\n",
      "300 done\n",
      "310 done\n",
      "320 done\n",
      "330 done\n",
      "340 done\n",
      "350 done\n",
      "360 done\n",
      "370 done\n",
      "380 done\n",
      "390 done\n",
      "400 done\n",
      "[!ERROR] C:\\Users\\wilso\\Desktop\\MDP-Autonomous-Robot-Grp-33\\RPI\\Image-rec-EfficientDet\\data\\train\\d3.jpeg\n",
      "410 done\n",
      "420 done\n",
      "430 done\n",
      "440 done\n",
      "450 done\n",
      "460 done\n",
      "470 done\n",
      "480 done\n",
      "490 done\n",
      "500 done\n",
      "510 done\n",
      "520 done\n",
      "530 done\n",
      "540 done\n",
      "550 done\n",
      "560 done\n",
      "570 done\n",
      "580 done\n",
      "590 done\n",
      "600 done\n",
      "610 done\n",
      "620 done\n",
      "630 done\n",
      "640 done\n",
      "650 done\n",
      "660 done\n",
      "670 done\n",
      "680 done\n",
      "690 done\n",
      "700 done\n",
      "710 done\n",
      "720 done\n",
      "730 done\n",
      "740 done\n",
      "750 done\n",
      "760 done\n",
      "770 done\n",
      "780 done\n",
      "790 done\n",
      "800 done\n",
      "810 done\n",
      "820 done\n",
      "830 done\n",
      "840 done\n",
      "850 done\n",
      "860 done\n",
      "870 done\n",
      "880 done\n",
      "890 done\n",
      "900 done\n",
      "910 done\n",
      "920 done\n",
      "930 done\n",
      "940 done\n",
      "950 done\n",
      "960 done\n",
      "970 done\n",
      "980 done\n",
      "990 done\n",
      "1000 done\n",
      "1010 done\n",
      "1020 done\n",
      "1030 done\n",
      "1040 done\n",
      "1050 done\n",
      "1060 done\n",
      "1070 done\n",
      "1080 done\n",
      "1090 done\n",
      "1100 done\n",
      "1110 done\n",
      "1120 done\n",
      "1130 done\n",
      "1140 done\n",
      "1150 done\n",
      "1160 done\n",
      "1170 done\n",
      "1180 done\n",
      "1190 done\n",
      "1200 done\n",
      "1210 done\n",
      "1220 done\n",
      "1230 done\n",
      "1240 done\n",
      "1250 done\n",
      "1260 done\n",
      "1270 done\n",
      "1280 done\n",
      "1290 done\n",
      "1300 done\n",
      "1310 done\n",
      "1320 done\n",
      "1330 done\n",
      "1340 done\n",
      "1350 done\n",
      "1360 done\n",
      "Exist pth\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exists_pth' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-6c4c95338343>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Exist pth\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msrc_pth\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexists_pth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msrc_pth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrsplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'exists_pth' is not defined"
     ]
    }
   ],
   "source": [
    "### Rename files\n",
    "import glob\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "TRAIN_PATH = Path(r\"C:\\Users\\wilso\\Desktop\\MDP-Autonomous-Robot-Grp-33\\RPI\\Image-rec-EfficientDet\\data\\train\")\n",
    "IMAGE_PATHS = list(paths.list_images(TRAIN_PATH))\n",
    "print(len(IMAGE_PATHS))\n",
    "\n",
    "exist_pths = []\n",
    "count = 1\n",
    "dst_dir = r\"C:\\Users\\wilso\\Desktop\\MDP-Autonomous-Robot-Grp-33\\RPI\\Image-rec-EfficientDet\\data\\train\"\n",
    "for idx, src_pth in enumerate(IMAGE_PATHS):\n",
    "    try:\n",
    "        ext = src_pth.rsplit(\".\")[-1]\n",
    "        xml_pth = src_pth.replace(ext, \"xml\")\n",
    "\n",
    "        os.rename(src_pth, os.path.join(dst_dir, str(count) + \".\" + ext))\n",
    "        os.rename(xml_pth, os.path.join(dst_dir, str(count) + \".xml\"))\n",
    "    except:\n",
    "        print(\"[!ERROR]\", src_pth)\n",
    "        exist_pths.append(src_pth)\n",
    "\n",
    "    if idx%10 == 0:\n",
    "        print(idx, \"done\")\n",
    "    count += 1\n",
    "\n",
    "print(\"Exist pth\")\n",
    "for idx, src_pth in enumerate(exist_pths):\n",
    "    try:\n",
    "        ext = src_pth.rsplit(\".\")[-1]\n",
    "        xml_pth = src_pth.replace(ext, \"xml\")\n",
    "\n",
    "        os.rename(src_pth, os.path.join(dst_dir, str(count) + \".\" + ext))\n",
    "        os.rename(xml_pth, os.path.join(dst_dir, str(count) + \".xml\"))\n",
    "    except:\n",
    "        print(\"[!ERROR]\", src_pth)\n",
    "\n",
    "    if idx%10 == 0:\n",
    "        print(idx, \"done\")\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper Functions ###\n",
    "def read_xml(path):\n",
    "    # load the annotation file, build the soup, and initialize our\n",
    "    # list of ground-truth bounding boxes\n",
    "    contents = open(path).read()\n",
    "    soup = BeautifulSoup(contents, \"html.parser\")\n",
    "    gtBoxes = []\n",
    "\n",
    "    # extract the image dimensions\n",
    "    w = int(soup.find(\"width\").string)\n",
    "    h = int(soup.find(\"height\").string)\n",
    "\n",
    "    # loop over all 'object' elements\n",
    "    for o in soup.find_all(\"object\"):\n",
    "        # extract the label and bounding box coordinates\n",
    "        label = o.find(\"name\").string\n",
    "        xMin = int(o.find(\"xmin\").string)\n",
    "        yMin = int(o.find(\"ymin\").string)\n",
    "        xMax = int(o.find(\"xmax\").string)\n",
    "        yMax = int(o.find(\"ymax\").string)\n",
    "\n",
    "        # truncate any bounding box coordinates that may fall\n",
    "        # outside the boundaries of the image\n",
    "        xMin = max(0, xMin)\n",
    "        yMin = max(0, yMin)\n",
    "        xMax = min(w, xMax)\n",
    "        yMax = min(h, yMax)\n",
    "        \n",
    "        # update our list of ground-truth bounding boxes\n",
    "        gtBoxes.append((xMin, yMin, xMax, yMax))\n",
    "    return gtBoxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ### Display bbox ###\n",
    "# image_path_sample_ls = IMAGE_PATHS[100:102]\n",
    "\n",
    "# prev_img_class = \"\"\n",
    "# for (i, image_path) in enumerate(image_path_sample_ls):\n",
    "#     img_fname = image_path.split(os.path.sep)[-1]\n",
    "#     img_fname = img_fname[:img_fname.rfind(\".\")]\n",
    "#     img_class = image_path.split(os.path.sep)[-3]\n",
    "#     if img_class != prev_img_class:\n",
    "#         print(img_class) \n",
    "    \n",
    "#     img_path = Path(os.path.join(TRAIN_PATH, img_class, \"images\", img_fname + \".jpg\"))\n",
    "#     bbox_path = Path(os.path.join(TRAIN_PATH, img_class, \"annotations\", img_fname + \".xml\"))\n",
    "\n",
    "#     img = Image.open(img_path) \n",
    "#     draw = ImageDraw.Draw(img)\n",
    "\n",
    "#     bboxes = read_xml(bbox_path)\n",
    "#     # The box contains the upper left corner (x, y) coordinates then width and height. \n",
    "#     # So we need to change these to (x1, y1) and (x2, y2) where they are the upper left and lower right corners \n",
    "#     for bbox in bboxes: \n",
    "#         x, y, w, h = bbox \n",
    "#         draw.rectangle(bbox, outline=\"red\", width=3) \n",
    "#     img = img.resize((250, 250))\n",
    "#     prev_img_class = img_class\n",
    "#     display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = Path(r\"C:\\Users\\wilso\\Desktop\\MDP-Autonomous-Robot-Grp-33\\RPI\\Image-rec-EfficientDet\\other data\\MDP Grp 31 Data\\train\")\n",
    "IMAGE_PATHS = list(paths.list_images(TRAIN_PATH))\n",
    "\n",
    "### Display bbox ###\n",
    "image_path_sample_ls = IMAGE_PATHS[100:102]\n",
    "\n",
    "prev_img_class = \"\"\n",
    "for (i, image_path) in enumerate(image_path_sample_ls):\n",
    "    img_fname = image_path.split(os.path.sep)[-1]\n",
    "    img_fname = img_fname[:img_fname.rfind(\".\")]\n",
    "#     img_class = image_path.split(os.path.sep)[-3]\n",
    "#     if img_class != prev_img_class:\n",
    "#         print(img_class) \n",
    "    \n",
    "    img_path = Path(os.path.join(TRAIN_PATH, img_fname + \".jpg\"))\n",
    "    bbox_path = Path(os.path.join(TRAIN_PATH, img_fname + \".xml\"))\n",
    "\n",
    "    img = Image.open(img_path) \n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    bboxes = read_xml(bbox_path)\n",
    "    # The box contains the upper left corner (x, y) coordinates then width and height. \n",
    "    # So we need to change these to (x1, y1) and (x2, y2) where they are the upper left and lower right corners \n",
    "    for bbox in bboxes: \n",
    "        x, y, w, h = bbox \n",
    "        draw.rectangle(bbox, outline=\"red\", width=3) \n",
    "    img = img.resize((250, 250))\n",
    "    prev_img_class = img_class\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Config ###\n",
    "IMG_SIZE = 256 # Using small image size to train faster, will try bigger images for better performance.\n",
    "\n",
    "### Hyperparameters ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EfficientDet from [link](https://github.com/rwightman/efficientdet-pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain\n",
    "from effdet.efficientdet import HeadNet\n",
    "\n",
    "def get_train_efficientdet():\n",
    "    # Get the model's config\n",
    "    config = get_efficientdet_config(\"tf_efficientdet_d1\")\n",
    "    # Create the model\n",
    "    net = EfficientDet(config, pretrained_backbone=False)\n",
    "    # Load pretrained EfficientDet weights\n",
    "    checkpoint = torch.load(\"path/to/weights\")\n",
    "    net.load_state_dict(checkpoint)\n",
    "    # Change the number of classes to 1\n",
    "    config.num_classes = 1\n",
    "    config.image_size = 256\n",
    "    # Add the head with the updated parameters\n",
    "    head = HeadNet(\n",
    "        config,\n",
    "        num_outputs=config.num_classes,\n",
    "        norm_kwargs=dict(eps=0.001, momentum=0.01),\n",
    "    )\n",
    "    # Attach the head\n",
    "    net.class_net = head\n",
    "    return DetBenchTrain(net, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def get_train_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.RandomSizedCrop(min_max_height=(800, 800), height=IMG_SIZE, width=IMG_SIZE, p=0.5),\n",
    "            A.OneOf([\n",
    "                A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, \n",
    "                                     val_shift_limit=0.2, p=0.9),\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.2, \n",
    "                                           contrast_limit=0.2, p=0.9),\n",
    "            ],p=0.9),\n",
    "            A.ToGray(p=0.01),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.Resize(height=256, width=256, p=1),\n",
    "            A.Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=0, p=0.5),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], \n",
    "        p=1.0, \n",
    "        bbox_params=A.BboxParams(\n",
    "            format='pascal_voc',\n",
    "            min_area=0, \n",
    "            min_visibility=0,\n",
    "            label_fields=['labels']\n",
    "        )\n",
    "    )\n",
    "\n",
    "def get_valid_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(height=IMG_SIZE, width=IMG_SIZE, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], \n",
    "        p=1.0, \n",
    "        bbox_params=A.BboxParams(\n",
    "            format='pascal_voc',\n",
    "            min_area=0, \n",
    "            min_visibility=0,\n",
    "            label_fields=['labels']\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def get_test_transforms():\n",
    "    return A.Compose([\n",
    "            A.Resize(height=IMG_SIZE, width=IMG_SIZE, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MDPDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df = None, mode = \"train\", image_dir = \"\", transforms=None):\n",
    "        super().__init__()\n",
    "        if df is not None:\n",
    "            self.df = df.copy()\n",
    "            self.image_ids = df['image_id'].unique()\n",
    "        else:\n",
    "            # Test case\n",
    "            self.df = None\n",
    "            self.image_ids = [p.stem for p in Path(image_dir).glob(\"*.jpg\")]\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "        self.mode = mode\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "\n",
    "        image_id = self.image_ids[index]\n",
    "        # Could be one or many rows.\n",
    "\n",
    "        image = Image.open(f'{self.image_dir}/{image_id}.jpg').convert(\"RGB\")\n",
    "        # Convert to Numpy array\n",
    "        image = np.array(image)\n",
    "        image = image / 255.0\n",
    "        image = image.astype(np.float32)\n",
    "\n",
    "\n",
    "        if self.mode != \"test\":\n",
    "    \n",
    "            records = self.df[self.df['image_id'] == image_id]\n",
    "\n",
    "            area = records[\"area\"].values\n",
    "            area = torch.as_tensor(area, dtype=torch.float32)\n",
    "\n",
    "            boxes = records[[\"xmin\", \"ymin\", \"xmax\", \"ymax\"]].values\n",
    "\n",
    "            # there is only one class, so always 1.\n",
    "            labels = torch.ones((records.shape[0],), dtype=torch.int64)\n",
    "            \n",
    "            # suppose all instances are not crowd.\n",
    "            iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n",
    "        \n",
    "            target = {}\n",
    "            target['boxes'] = boxes\n",
    "            target['labels'] = labels\n",
    "            # target['masks'] = None\n",
    "            target['image_id'] = torch.tensor([index])\n",
    "            target['area'] = area\n",
    "            target['iscrowd'] = iscrowd\n",
    "            # These are needed as well by the efficientdet model.\n",
    "            target['img_size'] = torch.tensor([(IMG_SIZE, IMG_SIZE)])\n",
    "            target['img_scale'] = torch.tensor([1.])\n",
    "        \n",
    "        else:\n",
    "\n",
    "            # test dataset must have some values so that transforms work.\n",
    "            target = {'cls': torch.as_tensor([[0]], dtype=torch.float32),\n",
    "                      'bbox': torch.as_tensor([[0, 0, 0, 0]], dtype=torch.float32),\n",
    "                      'img_size': torch.tensor([(IMG_SIZE, IMG_SIZE)]),\n",
    "                      'img_scale': torch.tensor([1.])}\n",
    "\n",
    "\n",
    "        if self.mode != \"test\":\n",
    "            if self.transforms:\n",
    "                sample = {\n",
    "                    'image': image,\n",
    "                    'bboxes': target['boxes'],\n",
    "                    'labels': labels\n",
    "                }\n",
    "                if len(sample['bboxes']) > 0:\n",
    "                    # Apply some augmentation on the fly. \n",
    "                    sample = self.transforms(**sample)\n",
    "                    image = sample['image']\n",
    "                    boxes = sample['bboxes']\n",
    "                    # Need yxyx format for EfficientDet.\n",
    "                    target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*boxes)))).permute(1, 0)\n",
    "\n",
    "        else:\n",
    "            sample = {\n",
    "                'image': image,\n",
    "                'bbox': target['bbox'],\n",
    "                'cls': target['cls']\n",
    "            }\n",
    "            image = self.transforms(**sample)['image']\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some processing to add some metadata to the labels DataFrame\n",
    "processed_train_labels_df = pd.read_csv(r\"C:\\Users\\wilso\\Desktop\\MDP-Autonomous-Robot-Grp-33\\RPI\\Image-rec-EfficientDet\\data\\csv\\train.csv\")\n",
    "\n",
    "# train_labels_df['bbox'] = train_labels_df['bbox'].apply(eval)\n",
    "\n",
    "# x = []\n",
    "# y = []\n",
    "# w = []\n",
    "# h = []\n",
    "# for bbox in train_labels_df['bbox']:\n",
    "#     x.append(bbox[0])\n",
    "#     y.append(bbox[1])\n",
    "#     w.append(bbox[2])\n",
    "#     h.append(bbox[3])\n",
    "    \n",
    "# processed_train_labels_df = train_labels_df.copy()\n",
    "# processed_train_labels_df[\"x\"] = x\n",
    "# processed_train_labels_df[\"y\"] = y\n",
    "# processed_train_labels_df[\"w\"] = w\n",
    "# processed_train_labels_df[\"h\"] = h\n",
    "\n",
    "\n",
    "# processed_train_labels_df[\"area\"] = processed_train_labels_df[\"w\"] * processed_train_labels_df[\"h\"]\n",
    "# processed_train_labels_df[\"x2\"] = processed_train_labels_df[\"x\"] + processed_train_labels_df[\"w\"]\n",
    "# processed_train_labels_df[\"y2\"] = processed_train_labels_df[\"y\"] + processed_train_labels_df[\"h\"]\n",
    "\n",
    "# Create stratified folds, here using the source.\n",
    "# This isn't the most optimal way to do it but I will leave it to you \n",
    "# to find a better one. ;)\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "for fold, (train_index, valid_index) in enumerate(skf.split(processed_train_labels_df, \n",
    "                                                            y=processed_train_labels_df[\"filename\"])):\n",
    "    processed_train_labels_df.loc[valid_index, \"fold\"] = fold\n",
    "processed_train_labels_df.sample(2).T\n",
    "# We need to set a folder to get images\n",
    "TRAIN_IMG_FOLDER = r\"C:\\Users\\wilso\\Desktop\\MDP-Autonomous-Robot-Grp-33\\RPI\\Image-rec-EfficientDet\\data\\train\"\n",
    "\n",
    "# This is the transforms for the training phase\n",
    "train_transforms = get_train_transforms()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = MDPDataset(\n",
    "    \n",
    "    processed_train_labels_df, mode=\"train\", image_dir=TRAIN_IMG_FOLDER, transforms=train_transforms\n",
    ")\n",
    "\n",
    "image, target = train_dataset[0]\n",
    "print(image)\n",
    "print(target)\n",
    "import matplotlib.pylab as plt\n",
    "from PIL import Image, ImageDraw\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def plot_image_with_bboxes(img_id, df):\n",
    "    img_path = Path(TRAIN_IMG_FOLDER + f\"/{img_id}.jpg\")\n",
    "    img = Image.open(img_path)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    bboxes = df.loc[lambda df: df[\"filename\"] == img_id, \"bbox\"]\n",
    "    # The box contains the upper left corner (x, y) coordinates then width and height.\n",
    "    # So we need to change these to (x1, y1) and (x2, y2) where they are the upper \n",
    "    # left and lower right corners\n",
    "    for bbox in bboxes:\n",
    "        x, y, w, h = bbox\n",
    "        transformed_bbox = [x, y, x + w, y + h]\n",
    "        draw.rectangle(transformed_bbox, outline=\"red\", width=3)\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "plot_image_with_bboxes(\"1\", processed_train_labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain\n",
    "from effdet.efficientdet import HeadNet\n",
    "\n",
    "\n",
    "def get_train_efficientdet():\n",
    "    config = get_efficientdet_config('tf_efficientdet_d1')\n",
    "    net = EfficientDet(config, pretrained_backbone=False)\n",
    "    checkpoint = torch.load('../input/efficientdet/efficientdet_d5-ef44aea8.pth')\n",
    "    net.load_state_dict(checkpoint)\n",
    "    config.num_classes = 1\n",
    "    config.image_size = IMG_SIZE\n",
    "    net.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n",
    "    return DetBenchTrain(net, config)\n",
    "from pytorch_lightning import LightningModule\n",
    "\n",
    "class MDPModel(LightningModule):\n",
    "    \n",
    "    def __init__(self, df, fold):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.train_df = self.df.loc[lambda df: df[\"fold\"] != fold]\n",
    "        self.valid_df = self.df.loc[lambda df: df[\"fold\"] == fold]\n",
    "        self.image_dir = TRAIN_IMG_FOLDER\n",
    "        self.model = get_train_efficientdet()\n",
    "        self.num_workers = 4\n",
    "        self.batch_size = 8\n",
    "    \n",
    "    \n",
    "    def forward(self, image, target):\n",
    "        return self.model(image, target)\n",
    "    \n",
    "\n",
    "# Create a model for one fold.\n",
    "model = MDPModel(processed_train_labels_df, fold=0)\n",
    "model\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# Let's add the train and validation data loaders.\n",
    "\n",
    "def train_dataloader(self):\n",
    "    train_transforms = get_train_transforms()\n",
    "    train_dataset = MDPDataset(\n",
    "        self.train_df, image_dir=self.image_dir, transforms=train_transforms\n",
    "    )\n",
    "    return DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=self.batch_size,\n",
    "        sampler=RandomSampler(train_dataset),\n",
    "        pin_memory=False,\n",
    "        drop_last=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=self.num_workers,\n",
    "    )\n",
    "\n",
    "def val_dataloader(self):\n",
    "    valid_transforms = get_train_transforms()\n",
    "    valid_dataset = MDPDataset(\n",
    "        self.valid_df, image_dir=self.image_dir, transforms=valid_transforms\n",
    "    )\n",
    "    valid_dataloader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=self.batch_size,\n",
    "        sampler=SequentialSampler(valid_dataset),\n",
    "        pin_memory=False,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=self.num_workers,\n",
    "    )\n",
    "    iou_types = [\"bbox\"]\n",
    "    # Had to comment these since the evaluation doesn't work yet.\n",
    "    # More on this in the next section.\n",
    "    # coco = convert_to_coco_api(valid_dataset)\n",
    "    # self.coco_evaluator = CocoEvaluator(coco, iou_types)\n",
    "    return valid_dataloader\n",
    "\n",
    "\n",
    "\n",
    "MDPModel.train_dataloader = train_dataloader\n",
    "MDPModel.val_dataloader = val_dataloader\n",
    "def training_step(self, batch, batch_idx):\n",
    "    images, targets = batch\n",
    "    targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "    # separate losses\n",
    "    images = torch.stack(images).float()\n",
    "    targets2 = {}\n",
    "    targets2[\"bbox\"] = [\n",
    "        target[\"boxes\"].float() for target in targets\n",
    "    ]  # variable number of instances, so the entire structure can be forced to tensor\n",
    "    targets2[\"cls\"] = [target[\"labels\"].float() for target in targets]\n",
    "    targets2[\"image_id\"] = torch.tensor(\n",
    "        [target[\"image_id\"] for target in targets]\n",
    "    ).float()\n",
    "    targets2[\"img_scale\"] = torch.tensor(\n",
    "        [target[\"img_scale\"] for target in targets]\n",
    "    ).float()\n",
    "    targets2[\"img_size\"] = torch.tensor(\n",
    "        [(IMG_SIZE, IMG_SIZE) for target in targets]\n",
    "    ).float()\n",
    "    losses_dict = self.model(images, targets2)\n",
    "\n",
    "    return {\"loss\": losses_dict[\"loss\"], \"log\": losses_dict}\n",
    "\n",
    "def validation_step(self, batch, batch_idx):\n",
    "    images, targets = batch\n",
    "    targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "    # separate losses\n",
    "    images = torch.stack(images).float()\n",
    "    targets2 = {}\n",
    "    targets2[\"bbox\"] = [\n",
    "        target[\"boxes\"].float() for target in targets\n",
    "    ]  # variable number of instances, so the entire structure can be forced to tensor\n",
    "    targets2[\"cls\"] = [target[\"labels\"].float() for target in targets]\n",
    "    targets2[\"image_id\"] = torch.tensor(\n",
    "        [target[\"image_id\"] for target in targets]\n",
    "    ).float()\n",
    "    targets2[\"img_scale\"] = torch.tensor(\n",
    "        [target[\"img_scale\"] for target in targets], device=\"cuda\"\n",
    "    ).float()\n",
    "    targets2[\"img_size\"] = torch.tensor(\n",
    "        [(IMG_SIZE, IMG_SIZE) for target in targets], device=\"cuda\"\n",
    "    ).float()\n",
    "    losses_dict = self.model(images, targets2)\n",
    "    loss_val = losses_dict[\"loss\"]\n",
    "    detections = losses_dict[\"detections\"]\n",
    "    # Back to xyxy format.\n",
    "    detections[:, :, [1,0,3,2]] = detections[:, :, [0,1,2,3]]\n",
    "    # xywh to xyxy => not necessary.\n",
    "    # detections[:, :, 2] += detections[:, :, 0]\n",
    "    # detections[:, :, 3] += detections[:, :, 1]\n",
    "\n",
    "    res = {target[\"image_id\"].item(): {\n",
    "                'boxes': output[:, 0:4],\n",
    "                'scores': output[:, 4],\n",
    "                'labels': output[:, 5]}\n",
    "            for target, output in zip(targets, detections)}\n",
    "    # iou = self._calculate_iou(targets, res, IMG_SIZE)\n",
    "    # iou = torch.as_tensor(iou)\n",
    "    # self.coco_evaluator.update(res)\n",
    "    return {\"loss\": loss_val, \"log\": losses_dict}\n",
    "\n",
    "def validation_epoch_end(self, outputs):\n",
    "    # self.coco_evaluator.accumulate()\n",
    "    # self.coco_evaluator.summarize()\n",
    "    # coco main metric\n",
    "    # metric = self.coco_evaluator.coco_eval[\"bbox\"].stats[0]\n",
    "    # metric = torch.as_tensor(metric)\n",
    "    # tensorboard_logs = {\"main_score\": metric}\n",
    "    # return {\n",
    "    #     \"val_loss\": metric,\n",
    "    #     \"log\": tensorboard_logs,\n",
    "    #     \"progress_bar\": tensorboard_logs,\n",
    "    # }\n",
    "    pass\n",
    "\n",
    "def configure_optimizers(self):\n",
    "    return torch.optim.AdamW(self.model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "\n",
    "MDPModel.training_step = training_step\n",
    "# Had to comment these since the evaluation doesn't work yet. More on this \n",
    "# in the next section.\n",
    "# MDPModel.validation_step = validation_step\n",
    "# MDPModel.validation_epoch_end = validation_epoch_end\n",
    "MDPModel.configure_optimizers = configure_optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer, seed_everything, loggers\n",
    "\n",
    "seed_everything(314)\n",
    "# Create a model for one fold.\n",
    "# As an exercise, try doing it for the other folds and chaning the Trainer. ;)\n",
    "model = MDPModel(processed_train_labels_df, fold=0)\n",
    "logger = loggers.TensorBoardLogger(\"logs\", name=\"effdet-b5\", version=\"fold_0\")  \n",
    "trainer = Trainer(gpus=1, logger=logger, fast_dev_run=True)\n",
    "trainer.fit(model)\n",
    "torch.save(model.model.state_dict(), \"mdp_cv.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from effdet import DetBenchPredict\n",
    "\n",
    "\n",
    "def get_test_efficientdet(checkpoint_path):\n",
    "    config = get_efficientdet_config('tf_efficientdet_d1')\n",
    "    net = EfficientDet(config, pretrained_backbone=False)\n",
    "\n",
    "    config.num_classes = 1\n",
    "    config.image_size=IMG_SIZE\n",
    "    net.class_net = HeadNet(config, num_outputs=config.num_classes, \n",
    "                            norm_kwargs=dict(eps=.001, momentum=.01))\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    net.load_state_dict(checkpoint, strict=False)\n",
    "    net = DetBenchPredict(net, config)\n",
    "    return net\n",
    "\n",
    "\n",
    "\n",
    "def make_predictions(model, images, score_threshold=0.22):\n",
    "    images = torch.stack(images).float()\n",
    "    predictions = []\n",
    "    targets = torch.tensor([1]*images.shape[0])\n",
    "    img_scales = torch.tensor(\n",
    "        [1 for target in targets]\n",
    "    ).float()\n",
    "    img_sizes = torch.tensor(\n",
    "        [(IMG_SIZE, IMG_SIZE) for target in targets]\n",
    "    ).float()\n",
    "    with torch.no_grad():\n",
    "        detections = model(images, img_scales, img_sizes)\n",
    "        for i in range(images.shape[0]):\n",
    "            pred = detections[i].detach().cpu().numpy()\n",
    "            boxes = pred[:,:4]    \n",
    "            scores = pred[:,4]\n",
    "            indexes = np.where(scores > score_threshold)[0]\n",
    "            boxes = boxes[indexes]\n",
    "            boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n",
    "            boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n",
    "            predictions.append({\n",
    "                'boxes': boxes[indexes],\n",
    "                'scores': scores[indexes],\n",
    "            })\n",
    "    return predictions\n",
    "TEST_IMG_FOLDER = \"../input/global-MDP-detection/test\"\n",
    "\n",
    "\n",
    "test_model = get_test_efficientdet(\"mdp_cv.pth\")\n",
    "\n",
    "test_dataset = MDPDataset(\n",
    "    \n",
    "    None, mode=\"test\", image_dir=TEST_IMG_FOLDER, transforms=get_test_transforms()\n",
    ")\n",
    "\n",
    "image, _ = test_dataset[0]\n",
    "\n",
    "predictions = make_predictions(test_model, [image])\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "lr = 0.0002\n",
    "batch_size = 64\n",
    "image_size = 64 #\n",
    "channels_img = 1\n",
    "channels_noise = 256\n",
    "num_epochs = 10\n",
    "\n",
    "features_d = 16 # may want to set larger\n",
    "features_g = 16 # may want to set larger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mdp-cv]",
   "language": "python",
   "name": "conda-env-mdp-cv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
