{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import argparse\n",
    "import os \n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageDraw\n",
    "from bs4 import BeautifulSoup\n",
    "from imutils import paths\n",
    "\n",
    "import threading\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
    "import torch.optim as optim # All optimization algos, SGD, Adam, etc\n",
    "import torchvision.transforms as transforms # Transformations to perform on data set, in addition to obspy \n",
    "from torch.utils.data import DataLoader # For dataset management and to create mini batches\n",
    "from pprint import pprint\n",
    "\n",
    "import torchvision.datasets as datasets # Standard datasets to test out model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_CLASSES = []\n",
    "\n",
    "colors = [(39, 129, 113), (164, 80, 133), (83, 122, 114), (99, 81, 172), (95, 56, 104), (37, 84, 86), (14, 89, 122),\n",
    "          (80, 7, 65), (10, 102, 25), (90, 185, 109), (106, 110, 132), (169, 158, 85), (188, 185, 26), (103, 1, 17),\n",
    "          (82, 144, 81), (92, 7, 184), (49, 81, 155), (179, 177, 69), (93, 187, 158), (13, 39, 73), (12, 50, 60),\n",
    "          (16, 179, 33), (112, 69, 165), (15, 139, 63), (33, 191, 159), (182, 173, 32), (34, 113, 133), (90, 135, 34),\n",
    "          (53, 34, 86), (141, 35, 190), (6, 171, 8), (118, 76, 112), (89, 60, 55), (15, 54, 88), (112, 75, 181),\n",
    "          (42, 147, 38), (138, 52, 63), (128, 65, 149), (106, 103, 24), (168, 33, 45), (28, 136, 135), (86, 91, 108),\n",
    "          (52, 11, 76), (142, 6, 189), (57, 81, 168), (55, 19, 148), (182, 101, 89), (44, 65, 179), (1, 33, 26),\n",
    "          (122, 164, 26), (70, 63, 134), (137, 106, 82), (120, 118, 52), (129, 74, 42), (182, 147, 112), (22, 157, 50),\n",
    "          (56, 50, 20), (2, 22, 177), (156, 100, 106), (21, 35, 42), (13, 8, 121), (142, 92, 28), (45, 118, 33),\n",
    "          (105, 118, 30), (7, 185, 124), (46, 34, 146), (105, 184, 169), (22, 18, 5), (147, 71, 73), (181, 64, 91),\n",
    "          (31, 39, 184), (164, 179, 33), (96, 50, 18), (95, 15, 106), (113, 68, 54), (136, 116, 112), (119, 139, 130),\n",
    "          (31, 139, 34), (66, 6, 127), (62, 39, 2), (49, 99, 180), (49, 119, 155), (153, 50, 183), (125, 38, 3),\n",
    "          (129, 87, 143), (49, 87, 40), (128, 62, 120), (73, 85, 148), (28, 144, 118), (29, 9, 24), (175, 45, 108),\n",
    "          (81, 175, 64), (178, 19, 157), (74, 188, 190), (18, 114, 2), (62, 128, 96), (21, 3, 150), (0, 6, 95),\n",
    "          (2, 20, 184), (122, 37, 185)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1362\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2710"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_PATH = Path(r\"C:\\Users\\wilso\\Desktop\\MDP-Autonomous-Robot-Grp-33\\RPI\\Image-rec-EfficientDet\\data\\train\")\n",
    "IMAGE_PATHS = list(paths.list_images(TRAIN_PATH))\n",
    "print(len(IMAGE_PATHS))\n",
    "len(list(TRAIN_PATH.glob(\"*\")))\n",
    "# DATA_PATH = r\"C:\\Users\\wilso\\Desktop\\ML-Earthquake-source-physics-research\\src\\data extraction\\wilson\\data_ISC\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EfficientDet from [link](https://github.com/signatrix/efficientdet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain\n",
    "from effdet.efficientdet import HeadNet\n",
    "\n",
    "def get_train_efficientdet():\n",
    "    # Get the model's config\n",
    "    config = get_efficientdet_config(\"tf_efficientdet_d5\")\n",
    "    # Create the model\n",
    "    net = EfficientDet(config, pretrained_backbone=False)\n",
    "    # Load pretrained EfficientDet weights\n",
    "    checkpoint = torch.load(r\"C:\\Users\\wilso\\Desktop\\MDP-Autonomous-Robot-Grp-33\\RPI\\Image-rec-EfficientDet\\effdet\\model\\efficientdet_d5-ef44aea8.pth\")\n",
    "    net.load_state_dict(checkpoint)\n",
    "    # Change the number of classes to 1\n",
    "    config.num_classes = 1\n",
    "    config.image_size = 256\n",
    "    # Add the head with the updated parameters\n",
    "    head = HeadNet(\n",
    "        config,\n",
    "        num_outputs=config.num_classes,\n",
    "        norm_kwargs=dict(eps=0.001, momentum=0.01),\n",
    "    )\n",
    "    # Attach the head\n",
    "    net.class_net = head\n",
    "    return DetBenchTrain(net, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def get_train_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.RandomSizedCrop(min_max_height=(20, 20), height=IMG_SIZE, width=IMG_SIZE, p=0.5),\n",
    "            A.OneOf([\n",
    "                A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, \n",
    "                                     val_shift_limit=0.2, p=0.9),\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.2, \n",
    "                                           contrast_limit=0.2, p=0.9),\n",
    "            ],p=0.9),\n",
    "            A.ToGray(p=0.01),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.Resize(height=256, width=256, p=1),\n",
    "            A.Cutout(num_holes=8, max_h_size=64, max_w_size=64, fill_value=0, p=0.5),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], \n",
    "        p=1.0, \n",
    "        bbox_params=A.BboxParams(\n",
    "            format='pascal_voc',\n",
    "            min_area=0, \n",
    "            min_visibility=0,\n",
    "            label_fields=['labels']\n",
    "        )\n",
    "    )\n",
    "\n",
    "def get_valid_transforms():\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.Resize(height=IMG_SIZE, width=IMG_SIZE, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], \n",
    "        p=1.0, \n",
    "        bbox_params=A.BboxParams(\n",
    "            format='pascal_voc',\n",
    "            min_area=0, \n",
    "            min_visibility=0,\n",
    "            label_fields=['labels']\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def get_test_transforms():\n",
    "    return A.Compose([\n",
    "            A.Resize(height=IMG_SIZE, width=IMG_SIZE, p=1.0),\n",
    "            ToTensorV2(p=1.0),\n",
    "        ], p=1.0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MDPDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df = None, mode = \"train\", image_dir = \"\", transforms=None):\n",
    "        super().__init__()\n",
    "        if df is not None:\n",
    "            self.df = df.copy()\n",
    "            self.image_ids = df['image_id'].unique()\n",
    "        else:\n",
    "            # Test case\n",
    "            self.df = None\n",
    "            self.image_ids = [p.stem for p in Path(image_dir).glob(\"*.jpg\")]\n",
    "        self.image_dir = image_dir\n",
    "        self.transforms = transforms\n",
    "        self.mode = mode\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "\n",
    "        image_id = self.image_ids[index]\n",
    "        # Could be one or many rows.\n",
    "        \n",
    "        print(f'{self.image_dir}')\n",
    "        print(f'{image_id}.jpg')\n",
    "        print(f'{self.image_dir}')\n",
    "        image = Image.open(f'{self.image_dir}/{image_id}.jpg').convert(\"RGB\")\n",
    "        # Convert to Numpy array\n",
    "        image = np.array(image)\n",
    "        image = image / 255.0\n",
    "        image = image.astype(np.float32)\n",
    "\n",
    "\n",
    "        if self.mode != \"test\":\n",
    "    \n",
    "            records = self.df[self.df['image_id'] == image_id]\n",
    "\n",
    "            area = records[\"area\"].values\n",
    "            area = torch.as_tensor(area, dtype=torch.float32)\n",
    "\n",
    "            boxes = records[[\"xmin\", \"ymin\", \"xmax\", \"ymax\"]].values\n",
    "\n",
    "            # there is only one class, so always 1.\n",
    "            labels = torch.ones((records.shape[0],), dtype=torch.int64)\n",
    "            \n",
    "            # suppose all instances are not crowd.\n",
    "            iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n",
    "        \n",
    "            target = {}\n",
    "            target['boxes'] = boxes\n",
    "            target['labels'] = labels\n",
    "            # target['masks'] = None\n",
    "            target['image_id'] = torch.tensor([index])\n",
    "            target['area'] = area\n",
    "            target['iscrowd'] = iscrowd\n",
    "            # These are needed as well by the efficientdet model.\n",
    "            target['img_size'] = torch.tensor([(IMG_SIZE, IMG_SIZE)])\n",
    "            target['img_scale'] = torch.tensor([1.])\n",
    "        \n",
    "        else:\n",
    "\n",
    "            # test dataset must have some values so that transforms work.\n",
    "            target = {'cls': torch.as_tensor([[0]], dtype=torch.float32),\n",
    "                      'bbox': torch.as_tensor([[0, 0, 0, 0]], dtype=torch.float32),\n",
    "                      'img_size': torch.tensor([(IMG_SIZE, IMG_SIZE)]),\n",
    "                      'img_scale': torch.tensor([1.])}\n",
    "\n",
    "\n",
    "        if self.mode != \"test\":\n",
    "            if self.transforms:\n",
    "                sample = {\n",
    "                    'image': image,\n",
    "                    'bboxes': target['boxes'],\n",
    "                    'labels': labels\n",
    "                }\n",
    "                if len(sample['bboxes']) > 0:\n",
    "                    # Apply some augmentation on the fly. \n",
    "                    sample = self.transforms(**sample)\n",
    "                    image = sample['image']\n",
    "                    boxes = sample['bboxes']\n",
    "                    # Need yxyx format for EfficientDet.\n",
    "                    target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*boxes)))).permute(1, 0)\n",
    "\n",
    "        else:\n",
    "            sample = {\n",
    "                'image': image,\n",
    "                'bbox': target['bbox'],\n",
    "                'cls': target['cls']\n",
    "            }\n",
    "            image = self.transforms(**sample)['image']\n",
    "\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>filename</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>class</th>\n",
       "      <th>xmin</th>\n",
       "      <th>ymin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>ymax</th>\n",
       "      <th>area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.1.2_img0.jpg</td>\n",
       "      <td>640</td>\n",
       "      <td>480</td>\n",
       "      <td>0</td>\n",
       "      <td>347</td>\n",
       "      <td>242</td>\n",
       "      <td>464</td>\n",
       "      <td>402</td>\n",
       "      <td>18720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>0.1.3_img3.jpg</td>\n",
       "      <td>640</td>\n",
       "      <td>480</td>\n",
       "      <td>0</td>\n",
       "      <td>351</td>\n",
       "      <td>277</td>\n",
       "      <td>429</td>\n",
       "      <td>404</td>\n",
       "      <td>9906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>20200828_091602.jpg</td>\n",
       "      <td>640</td>\n",
       "      <td>480</td>\n",
       "      <td>right_arrow</td>\n",
       "      <td>218</td>\n",
       "      <td>157</td>\n",
       "      <td>400</td>\n",
       "      <td>335</td>\n",
       "      <td>32396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>photo_2020-09-29_15-28-20.jpg</td>\n",
       "      <td>956</td>\n",
       "      <td>1276</td>\n",
       "      <td>9dd</td>\n",
       "      <td>299</td>\n",
       "      <td>459</td>\n",
       "      <td>446</td>\n",
       "      <td>674</td>\n",
       "      <td>31605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>photo_2020-09-29_15-28-21 (2).jpg</td>\n",
       "      <td>956</td>\n",
       "      <td>1276</td>\n",
       "      <td>9dd</td>\n",
       "      <td>162</td>\n",
       "      <td>417</td>\n",
       "      <td>307</td>\n",
       "      <td>652</td>\n",
       "      <td>34075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1360</th>\n",
       "      <td>995</td>\n",
       "      <td>photo_2020-09-29_15-28-16.jpg</td>\n",
       "      <td>956</td>\n",
       "      <td>1276</td>\n",
       "      <td>9dd</td>\n",
       "      <td>384</td>\n",
       "      <td>491</td>\n",
       "      <td>522</td>\n",
       "      <td>738</td>\n",
       "      <td>34086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1361</th>\n",
       "      <td>996</td>\n",
       "      <td>photo_2020-09-29_15-28-17 (2).jpg</td>\n",
       "      <td>956</td>\n",
       "      <td>1276</td>\n",
       "      <td>9dd</td>\n",
       "      <td>262</td>\n",
       "      <td>587</td>\n",
       "      <td>447</td>\n",
       "      <td>798</td>\n",
       "      <td>39035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1362</th>\n",
       "      <td>997</td>\n",
       "      <td>photo_2020-09-29_15-28-17.jpg</td>\n",
       "      <td>956</td>\n",
       "      <td>1276</td>\n",
       "      <td>9dd</td>\n",
       "      <td>391</td>\n",
       "      <td>487</td>\n",
       "      <td>539</td>\n",
       "      <td>736</td>\n",
       "      <td>36852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1363</th>\n",
       "      <td>998</td>\n",
       "      <td>photo_2020-09-29_15-28-18.jpg</td>\n",
       "      <td>956</td>\n",
       "      <td>1276</td>\n",
       "      <td>9dd</td>\n",
       "      <td>391</td>\n",
       "      <td>641</td>\n",
       "      <td>549</td>\n",
       "      <td>828</td>\n",
       "      <td>29546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1364</th>\n",
       "      <td>999</td>\n",
       "      <td>photo_2020-09-29_15-28-19.jpg</td>\n",
       "      <td>956</td>\n",
       "      <td>1276</td>\n",
       "      <td>9dd</td>\n",
       "      <td>280</td>\n",
       "      <td>496</td>\n",
       "      <td>416</td>\n",
       "      <td>719</td>\n",
       "      <td>30328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1365 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      image_id                           filename  width  height        class  \\\n",
       "0            1                     0.1.2_img0.jpg    640     480            0   \n",
       "1           10                     0.1.3_img3.jpg    640     480            0   \n",
       "2          100                20200828_091602.jpg    640     480  right_arrow   \n",
       "3         1000      photo_2020-09-29_15-28-20.jpg    956    1276          9dd   \n",
       "4         1001  photo_2020-09-29_15-28-21 (2).jpg    956    1276          9dd   \n",
       "...        ...                                ...    ...     ...          ...   \n",
       "1360       995      photo_2020-09-29_15-28-16.jpg    956    1276          9dd   \n",
       "1361       996  photo_2020-09-29_15-28-17 (2).jpg    956    1276          9dd   \n",
       "1362       997      photo_2020-09-29_15-28-17.jpg    956    1276          9dd   \n",
       "1363       998      photo_2020-09-29_15-28-18.jpg    956    1276          9dd   \n",
       "1364       999      photo_2020-09-29_15-28-19.jpg    956    1276          9dd   \n",
       "\n",
       "      xmin  ymin  xmax  ymax   area  \n",
       "0      347   242   464   402  18720  \n",
       "1      351   277   429   404   9906  \n",
       "2      218   157   400   335  32396  \n",
       "3      299   459   446   674  31605  \n",
       "4      162   417   307   652  34075  \n",
       "...    ...   ...   ...   ...    ...  \n",
       "1360   384   491   522   738  34086  \n",
       "1361   262   587   447   798  39035  \n",
       "1362   391   487   539   736  36852  \n",
       "1363   391   641   549   828  29546  \n",
       "1364   280   496   416   719  30328  \n",
       "\n",
       "[1365 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      image_id                           filename  width  height        class  \\\n",
      "0            1                     0.1.2_img0.jpg    640     480            0   \n",
      "1           10                     0.1.3_img3.jpg    640     480            0   \n",
      "2          100                20200828_091602.jpg    640     480  right_arrow   \n",
      "3         1000      photo_2020-09-29_15-28-20.jpg    956    1276          9dd   \n",
      "4         1001  photo_2020-09-29_15-28-21 (2).jpg    956    1276          9dd   \n",
      "...        ...                                ...    ...     ...          ...   \n",
      "1360       995      photo_2020-09-29_15-28-16.jpg    956    1276          9dd   \n",
      "1361       996  photo_2020-09-29_15-28-17 (2).jpg    956    1276          9dd   \n",
      "1362       997      photo_2020-09-29_15-28-17.jpg    956    1276          9dd   \n",
      "1363       998      photo_2020-09-29_15-28-18.jpg    956    1276          9dd   \n",
      "1364       999      photo_2020-09-29_15-28-19.jpg    956    1276          9dd   \n",
      "\n",
      "      xmin  ymin  xmax  ymax   area  fold  \n",
      "0      347   242   464   402  18720   0.0  \n",
      "1      351   277   429   404   9906   0.0  \n",
      "2      218   157   400   335  32396   0.0  \n",
      "3      299   459   446   674  31605   0.0  \n",
      "4      162   417   307   652  34075   0.0  \n",
      "...    ...   ...   ...   ...    ...   ...  \n",
      "1360   384   491   522   738  34086   1.0  \n",
      "1361   262   587   447   798  39035   1.0  \n",
      "1362   391   487   539   736  36852   1.0  \n",
      "1363   391   641   549   828  29546   1.0  \n",
      "1364   280   496   416   719  30328   1.0  \n",
      "\n",
      "[1365 rows x 11 columns]\n",
      "C:\\Users\\wilso\\Desktop\\MDP-Autonomous-Robot-Grp-33\\RPI\\Image-rec-EfficientDet\\data\\train\n",
      "1.jpg\n",
      "C:\\Users\\wilso\\Desktop\\MDP-Autonomous-Robot-Grp-33\\RPI\\Image-rec-EfficientDet\\data\\train\n",
      "tensor([[[0.7296, 0.7278, 0.7219,  ..., 0.6266, 0.6242, 0.6333],\n",
      "         [0.7370, 0.7339, 0.7360,  ..., 0.6288, 0.6282, 0.6362],\n",
      "         [0.7342, 0.7457, 0.7375,  ..., 0.6202, 0.6213, 0.6291],\n",
      "         ...,\n",
      "         [0.1868, 0.4847, 0.5147,  ..., 0.1965, 0.2026, 0.2000],\n",
      "         [0.3302, 0.5051, 0.5075,  ..., 0.2194, 0.2168, 0.2185],\n",
      "         [0.4664, 0.4980, 0.5104,  ..., 0.2183, 0.2125, 0.2210]],\n",
      "\n",
      "        [[0.6237, 0.6141, 0.6137,  ..., 0.5141, 0.5201, 0.5179],\n",
      "         [0.6266, 0.6291, 0.6289,  ..., 0.5150, 0.5222, 0.5169],\n",
      "         [0.6326, 0.6442, 0.6360,  ..., 0.5196, 0.5240, 0.5249],\n",
      "         ...,\n",
      "         [0.1212, 0.3797, 0.4105,  ..., 0.1373, 0.1461, 0.1365],\n",
      "         [0.2427, 0.4036, 0.4060,  ..., 0.1576, 0.1550, 0.1465],\n",
      "         [0.3659, 0.3953, 0.4001,  ..., 0.1465, 0.1540, 0.1482]],\n",
      "\n",
      "        [[0.4295, 0.4339, 0.4336,  ..., 0.3651, 0.3678, 0.3836],\n",
      "         [0.4457, 0.4364, 0.4438,  ..., 0.3771, 0.3656, 0.3803],\n",
      "         [0.4460, 0.4500, 0.4418,  ..., 0.3669, 0.3614, 0.3563],\n",
      "         ...,\n",
      "         [0.0663, 0.2547, 0.2818,  ..., 0.0895, 0.1043, 0.1003],\n",
      "         [0.1334, 0.2656, 0.2658,  ..., 0.1091, 0.1065, 0.1021],\n",
      "         [0.2266, 0.2547, 0.2633,  ..., 0.1120, 0.1143, 0.0907]]])\n",
      "{'boxes': tensor([[ 70.4000, 129.0667, 117.2000, 214.4000]], dtype=torch.float64), 'labels': tensor([1]), 'image_id': tensor([0]), 'area': tensor([18720.]), 'iscrowd': tensor([0]), 'img_size': tensor([[256, 256]]), 'img_scale': tensor([1.])}\n",
      "[!TEST] C:\\Users\\wilso\\Desktop\\MDP-Autonomous-Robot-Grp-33\\RPI\\Image-rec-EfficientDet\\data\\train\\1.jpg\n",
      "RangeIndex(start=0, stop=1365, step=1)\n",
      "      image_id                           filename  width  height        class  \\\n",
      "0            1                     0.1.2_img0.jpg    640     480            0   \n",
      "1           10                     0.1.3_img3.jpg    640     480            0   \n",
      "2          100                20200828_091602.jpg    640     480  right_arrow   \n",
      "3         1000      photo_2020-09-29_15-28-20.jpg    956    1276          9dd   \n",
      "4         1001  photo_2020-09-29_15-28-21 (2).jpg    956    1276          9dd   \n",
      "...        ...                                ...    ...     ...          ...   \n",
      "1360       995      photo_2020-09-29_15-28-16.jpg    956    1276          9dd   \n",
      "1361       996  photo_2020-09-29_15-28-17 (2).jpg    956    1276          9dd   \n",
      "1362       997      photo_2020-09-29_15-28-17.jpg    956    1276          9dd   \n",
      "1363       998      photo_2020-09-29_15-28-18.jpg    956    1276          9dd   \n",
      "1364       999      photo_2020-09-29_15-28-19.jpg    956    1276          9dd   \n",
      "\n",
      "      xmin  ymin  xmax  ymax   area  fold  \n",
      "0      347   242   464   402  18720   0.0  \n",
      "1      351   277   429   404   9906   0.0  \n",
      "2      218   157   400   335  32396   0.0  \n",
      "3      299   459   446   674  31605   0.0  \n",
      "4      162   417   307   652  34075   0.0  \n",
      "...    ...   ...   ...   ...    ...   ...  \n",
      "1360   384   491   522   738  34086   1.0  \n",
      "1361   262   587   447   798  39035   1.0  \n",
      "1362   391   487   539   736  36852   1.0  \n",
      "1363   391   641   549   828  29546   1.0  \n",
      "1364   280   496   416   719  30328   1.0  \n",
      "\n",
      "[1365 rows x 11 columns]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-67653fd3726f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m \u001b[0mplot_image_with_bboxes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprocessed_train_labels_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-22-67653fd3726f>\u001b[0m in \u001b[0;36mplot_image_with_bboxes\u001b[1;34m(img_id, df)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;31m# left and lower right corners\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mbbox\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbbox\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[0mtransformed_bbox\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mdraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrectangle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformed_bbox\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"red\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 4)"
     ]
    }
   ],
   "source": [
    "# Some processing to add some metadata to the labels DataFrame\n",
    "processed_train_labels_df = pd.read_csv(r\"C:\\Users\\wilso\\Desktop\\MDP-Autonomous-Robot-Grp-33\\RPI\\Image-rec-EfficientDet\\data\\csv\\train.csv\")\n",
    "display(processed_train_labels_df)\n",
    "# train_labels_df['bbox'] = train_labels_df['bbox'].apply(eval)\n",
    "\n",
    "# x = []\n",
    "# y = []\n",
    "# w = []\n",
    "# h = []\n",
    "# for bbox in train_labels_df['bbox']:\n",
    "#     x.append(bbox[0])\n",
    "#     y.append(bbox[1])\n",
    "#     w.append(bbox[2])\n",
    "#     h.append(bbox[3])\n",
    "    \n",
    "# processed_train_labels_df = train_labels_df.copy()\n",
    "# processed_train_labels_df[\"x\"] = x\n",
    "# processed_train_labels_df[\"y\"] = y\n",
    "# processed_train_labels_df[\"w\"] = w\n",
    "# processed_train_labels_df[\"h\"] = h\n",
    "\n",
    "\n",
    "# processed_train_labels_df[\"area\"] = processed_train_labels_df[\"w\"] * processed_train_labels_df[\"h\"]\n",
    "# processed_train_labels_df[\"x2\"] = processed_train_labels_df[\"x\"] + processed_train_labels_df[\"w\"]\n",
    "# processed_train_labels_df[\"y2\"] = processed_train_labels_df[\"y\"] + processed_train_labels_df[\"h\"]\n",
    "\n",
    "# Create stratified folds, here using the source.\n",
    "# This isn't the most optimal way to do it but I will leave it to you \n",
    "# to find a better one. ;)\n",
    "from sklearn.model_selection import KFold\n",
    "# skf = StratifiedKFold(n_splits=5)\n",
    "kf = KFold(n_splits=2, random_state=None, shuffle=False)\n",
    "for fold, (train_index, valid_index) in enumerate(kf.split(processed_train_labels_df[\"image_id\"])):\n",
    "    processed_train_labels_df.loc[valid_index, \"fold\"] = fold\n",
    "processed_train_labels_df.sample(2).T\n",
    "print(processed_train_labels_df)\n",
    "# We need to set a folder to get images\n",
    "TRAIN_IMG_FOLDER = r\"C:\\Users\\wilso\\Desktop\\MDP-Autonomous-Robot-Grp-33\\RPI\\Image-rec-EfficientDet\\data\\train\"\n",
    "\n",
    "# This is the transforms for the training phase\n",
    "train_transforms = get_train_transforms()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = MDPDataset(\n",
    "    \n",
    "    processed_train_labels_df, mode=\"train\", image_dir=TRAIN_IMG_FOLDER, transforms=train_transforms\n",
    ")\n",
    "\n",
    "\n",
    "image, target = train_dataset[0]\n",
    "print(image)\n",
    "print(target)\n",
    "import matplotlib.pylab as plt\n",
    "from PIL import Image, ImageDraw\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def plot_image_with_bboxes(img_id, df):\n",
    "    print(\"[!TEST]\", TRAIN_IMG_FOLDER + rf\"\\{img_id}.jpg\")\n",
    "    img_path = Path(TRAIN_IMG_FOLDER + rf\"\\{img_id}.jpg\")\n",
    "    img = Image.open(img_path)\n",
    "    draw = ImageDraw.Draw(img)\n",
    "#     bboxes = df.loc[df[\"image_id\"] == img_id]\n",
    "    print(df.index)\n",
    "    bboxes = df.loc[:,:]\n",
    "    print(bboxes)\n",
    "    # The box contains the upper left corner (x, y) coordinates then width and height.\n",
    "    # So we need to change these to (x1, y1) and (x2, y2) where they are the upper \n",
    "    # left and lower right corners\n",
    "    for bbox in bboxes:\n",
    "        x, y, x1, y1 = bbox\n",
    "        transformed_bbox = [x, y, x1, y1]\n",
    "        draw.rectangle(transformed_bbox, outline=\"red\", width=3)\n",
    "    return img\n",
    "\n",
    "\n",
    "\n",
    "plot_image_with_bboxes(\"1\", processed_train_labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wilso\\Anaconda3\\envs\\mdp-cv\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:37: UserWarning: Unsupported `ReduceOp` for distributed computing.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from effdet import get_efficientdet_config, EfficientDet, DetBenchTrain\n",
    "from effdet.efficientdet import HeadNet\n",
    "\n",
    "\n",
    "def get_train_efficientdet():\n",
    "    config = get_efficientdet_config('tf_efficientdet_d5')\n",
    "    net = EfficientDet(config, pretrained_backbone=False)\n",
    "    checkpoint = torch.load(r'C:\\Users\\wilso\\Desktop\\MDP-Autonomous-Robot-Grp-33\\RPI\\Image-rec-EfficientDet\\effdet\\model\\efficientdet_d5-ef44aea8.pth')\n",
    "    net.load_state_dict(checkpoint)\n",
    "    config.num_classes = 1\n",
    "    config.image_size = IMG_SIZE\n",
    "    net.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n",
    "    return DetBenchTrain(net, config)\n",
    "\n",
    "from pytorch_lightning import LightningModule\n",
    "\n",
    "class MDPModel(LightningModule):\n",
    "    \n",
    "    def __init__(self, df, fold):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.train_df = self.df.loc[lambda df: df[\"fold\"] != fold]\n",
    "        self.valid_df = self.df.loc[lambda df: df[\"fold\"] == fold]\n",
    "        self.image_dir = TRAIN_IMG_FOLDER\n",
    "        self.model = get_train_efficientdet()\n",
    "        self.num_workers = 4\n",
    "        self.batch_size = 8\n",
    "    \n",
    "    \n",
    "    def forward(self, image, target):\n",
    "        return self.model(image, target)\n",
    "    \n",
    "\n",
    "# Create a model for one fold.\n",
    "model = MDPModel(processed_train_labels_df, fold=0)\n",
    "model\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# Let's add the train and validation data loaders.\n",
    "\n",
    "def train_dataloader(self):\n",
    "    train_transforms = get_train_transforms()\n",
    "    train_dataset = MDPDataset(\n",
    "        self.train_df, image_dir=self.image_dir, transforms=train_transforms\n",
    "    )\n",
    "    return DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=self.batch_size,\n",
    "        sampler=RandomSampler(train_dataset),\n",
    "        pin_memory=False,\n",
    "        drop_last=True,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=self.num_workers,\n",
    "    )\n",
    "\n",
    "def val_dataloader(self):\n",
    "    valid_transforms = get_train_transforms()\n",
    "    valid_dataset = MDPDataset(\n",
    "        self.valid_df, image_dir=self.image_dir, transforms=valid_transforms\n",
    "    )\n",
    "    valid_dataloader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=self.batch_size,\n",
    "        sampler=SequentialSampler(valid_dataset),\n",
    "        pin_memory=False,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=self.num_workers,\n",
    "    )\n",
    "    iou_types = [\"bbox\"]\n",
    "    # Had to comment these since the evaluation doesn't work yet.\n",
    "    # More on this in the next section.\n",
    "    # coco = convert_to_coco_api(valid_dataset)\n",
    "    # self.coco_evaluator = CocoEvaluator(coco, iou_types)\n",
    "    return valid_dataloader\n",
    "\n",
    "\n",
    "\n",
    "MDPModel.train_dataloader = train_dataloader\n",
    "MDPModel.val_dataloader = val_dataloader\n",
    "def training_step(self, batch, batch_idx):\n",
    "    images, targets = batch\n",
    "    targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "    # separate losses\n",
    "    images = torch.stack(images).float()\n",
    "    targets2 = {}\n",
    "    targets2[\"bbox\"] = [\n",
    "        target[\"boxes\"].float() for target in targets\n",
    "    ]  # variable number of instances, so the entire structure can be forced to tensor\n",
    "    targets2[\"cls\"] = [target[\"labels\"].float() for target in targets]\n",
    "    targets2[\"image_id\"] = torch.tensor(\n",
    "        [target[\"image_id\"] for target in targets]\n",
    "    ).float()\n",
    "    targets2[\"img_scale\"] = torch.tensor(\n",
    "        [target[\"img_scale\"] for target in targets]\n",
    "    ).float()\n",
    "    targets2[\"img_size\"] = torch.tensor(\n",
    "        [(IMG_SIZE, IMG_SIZE) for target in targets]\n",
    "    ).float()\n",
    "    losses_dict = self.model(images, targets2)\n",
    "\n",
    "    return {\"loss\": losses_dict[\"loss\"], \"log\": losses_dict}\n",
    "\n",
    "def validation_step(self, batch, batch_idx):\n",
    "    images, targets = batch\n",
    "    targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "    # separate losses\n",
    "    images = torch.stack(images).float()\n",
    "    targets2 = {}\n",
    "    targets2[\"bbox\"] = [\n",
    "        target[\"boxes\"].float() for target in targets\n",
    "    ]  # variable number of instances, so the entire structure can be forced to tensor\n",
    "    targets2[\"cls\"] = [target[\"labels\"].float() for target in targets]\n",
    "    targets2[\"image_id\"] = torch.tensor(\n",
    "        [target[\"image_id\"] for target in targets]\n",
    "    ).float()\n",
    "    targets2[\"img_scale\"] = torch.tensor(\n",
    "        [target[\"img_scale\"] for target in targets], device=\"cuda\"\n",
    "    ).float()\n",
    "    targets2[\"img_size\"] = torch.tensor(\n",
    "        [(IMG_SIZE, IMG_SIZE) for target in targets], device=\"cuda\"\n",
    "    ).float()\n",
    "    losses_dict = self.model(images, targets2)\n",
    "    loss_val = losses_dict[\"loss\"]\n",
    "    detections = losses_dict[\"detections\"]\n",
    "    # Back to xyxy format.\n",
    "    detections[:, :, [1,0,3,2]] = detections[:, :, [0,1,2,3]]\n",
    "    # xywh to xyxy => not necessary.\n",
    "    # detections[:, :, 2] += detections[:, :, 0]\n",
    "    # detections[:, :, 3] += detections[:, :, 1]\n",
    "\n",
    "    res = {target[\"image_id\"].item(): {\n",
    "                'boxes': output[:, 0:4],\n",
    "                'scores': output[:, 4],\n",
    "                'labels': output[:, 5]}\n",
    "            for target, output in zip(targets, detections)}\n",
    "    # iou = self._calculate_iou(targets, res, IMG_SIZE)\n",
    "    # iou = torch.as_tensor(iou)\n",
    "    # self.coco_evaluator.update(res)\n",
    "    return {\"loss\": loss_val, \"log\": losses_dict}\n",
    "\n",
    "def validation_epoch_end(self, outputs):\n",
    "    # self.coco_evaluator.accumulate()\n",
    "    # self.coco_evaluator.summarize()\n",
    "    # coco main metric\n",
    "    # metric = self.coco_evaluator.coco_eval[\"bbox\"].stats[0]\n",
    "    # metric = torch.as_tensor(metric)\n",
    "    # tensorboard_logs = {\"main_score\": metric}\n",
    "    # return {\n",
    "    #     \"val_loss\": metric,\n",
    "    #     \"log\": tensorboard_logs,\n",
    "    #     \"progress_bar\": tensorboard_logs,\n",
    "    # }\n",
    "    pass\n",
    "\n",
    "def configure_optimizers(self):\n",
    "    return torch.optim.AdamW(self.model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "\n",
    "MDPModel.training_step = training_step\n",
    "# Had to comment these since the evaluation doesn't work yet. More on this \n",
    "# in the next section.\n",
    "# MDPModel.validation_step = validation_step\n",
    "# MDPModel.validation_epoch_end = validation_epoch_end\n",
    "MDPModel.configure_optimizers = configure_optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running in fast_dev_run mode: will run a full train, val and test loop using a single batch\n",
      "INFO:lightning:Running in fast_dev_run mode: will run a full train, val and test loop using a single batch\n",
      "GPU available: False, used: False\n",
      "INFO:lightning:GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning:TPU available: False, using: 0 TPU cores\n",
      "C:\\Users\\wilso\\Anaconda3\\envs\\mdp-cv\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:37: UserWarning: you passed in a val_dataloader but have no validation_step. Skipping validation loop\n",
      "  warnings.warn(*args, **kwargs)\n",
      "C:\\Users\\wilso\\Anaconda3\\envs\\mdp-cv\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:37: UserWarning: Could not log computational graph since the `model.example_input_array` attribute is not set or `input_array` was not given\n",
      "  warnings.warn(*args, **kwargs)\n",
      "\n",
      "  | Name  | Type          | Params\n",
      "----------------------------------------\n",
      "0 | model | DetBenchTrain | 33 M  \n",
      "INFO:lightning:\n",
      "  | Name  | Type          | Params\n",
      "----------------------------------------\n",
      "0 | model | DetBenchTrain | 33 M  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|                                                                         | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer, seed_everything, loggers\n",
    "\n",
    "seed_everything(314)\n",
    "# Create a model for one fold.\n",
    "# As an exercise, try doing it for the other folds and chaning the Trainer. ;)\n",
    "model = MDPModel(processed_train_labels_df, fold=0)\n",
    "logger = loggers.TensorBoardLogger(\"logs\", name=\"effdet-b5\", version=\"fold_0\")  \n",
    "trainer = Trainer(logger=logger, fast_dev_run=True)\n",
    "trainer.fit(model)\n",
    "torch.save(model.model.state_dict(), \"mdp_cv.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from effdet import DetBenchPredict\n",
    "\n",
    "\n",
    "def get_test_efficientdet(checkpoint_path):\n",
    "    config = get_efficientdet_config('tf_efficientdet_d1')\n",
    "    net = EfficientDet(config, pretrained_backbone=False)\n",
    "\n",
    "    config.num_classes = 1\n",
    "    config.image_size=IMG_SIZE\n",
    "    net.class_net = HeadNet(config, num_outputs=config.num_classes, \n",
    "                            norm_kwargs=dict(eps=.001, momentum=.01))\n",
    "\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    net.load_state_dict(checkpoint, strict=False)\n",
    "    net = DetBenchPredict(net, config)\n",
    "    return net\n",
    "\n",
    "\n",
    "\n",
    "def make_predictions(model, images, score_threshold=0.22):\n",
    "    images = torch.stack(images).float()\n",
    "    predictions = []\n",
    "    targets = torch.tensor([1]*images.shape[0])\n",
    "    img_scales = torch.tensor(\n",
    "        [1 for target in targets]\n",
    "    ).float()\n",
    "    img_sizes = torch.tensor(\n",
    "        [(IMG_SIZE, IMG_SIZE) for target in targets]\n",
    "    ).float()\n",
    "    with torch.no_grad():\n",
    "        detections = model(images, img_scales, img_sizes)\n",
    "        for i in range(images.shape[0]):\n",
    "            pred = detections[i].detach().cpu().numpy()\n",
    "            boxes = pred[:,:4]    \n",
    "            scores = pred[:,4]\n",
    "            indexes = np.where(scores > score_threshold)[0]\n",
    "            boxes = boxes[indexes]\n",
    "            boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n",
    "            boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n",
    "            predictions.append({\n",
    "                'boxes': boxes[indexes],\n",
    "                'scores': scores[indexes],\n",
    "            })\n",
    "    return predictions\n",
    "TEST_IMG_FOLDER = \"../input/global-MDP-detection/test\"\n",
    "\n",
    "\n",
    "test_model = get_test_efficientdet(\"mdp_cv.pth\")\n",
    "\n",
    "test_dataset = MDPDataset(\n",
    "    \n",
    "    None, mode=\"test\", image_dir=TEST_IMG_FOLDER, transforms=get_test_transforms()\n",
    ")\n",
    "\n",
    "image, _ = test_dataset[0]\n",
    "\n",
    "predictions = make_predictions(test_model, [image])\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "lr = 0.0002\n",
    "batch_size = 64\n",
    "image_size = 64 #\n",
    "channels_img = 1\n",
    "channels_noise = 256\n",
    "num_epochs = 10\n",
    "\n",
    "features_d = 16 # may want to set larger\n",
    "features_g = 16 # may want to set larger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mdp-cv]",
   "language": "python",
   "name": "conda-env-mdp-cv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
